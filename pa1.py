MODE = 'results'


## Logging and utility functions
import traceback
import time
import glob
import json
import pickle
import psutil
import os
import copy


NJOBS = psutil.cpu_count(logical=False)

def readjson(file):
    with open(file) as f:
        data = json.load(f)
    return data

def savejson(data,out):
    with open(out, 'w') as outfile:
        json.dump(data, outfile,indent=4)

def save_dict(path,datadict,mode='pickle'):
    SAVED = False
    try:
        if mode == 'pickle':
            with open(path, 'wb') as outfile:
                pickle.dump(datadict, outfile, protocol=pickle.HIGHEST_PROTOCOL)
            SAVED = True
        else:
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(datadict, f, ensure_ascii=False, indent=4)
            SAVED = True
    except Exception as e:
        print(f'Error saving to {path}')
        print(e)
    return SAVED

def log_error(e,log_file='log.txt'):
    traceback.print_exc(file=log_file)
    log_file.write('\n\n')
    log_file.flush()
def load_text_file(file_path):
    with open(file_path, 'r',encoding='utf8') as file:
        data = file.read()
    return data

if MODE=='generate':
    log_file = open('generation.log', 'w')
    from openai import OpenAI

    # Get the list of cities we have ground truth for
    cities = glob.glob('./ground-truth/*.txt')
    cities = [os.path.splitext(os.path.basename(x))[0].split('-')[-1] for x in cities]

    # We generate facts for each city with different veracities
    veracities = ['subtly fake','obviously fake','true','grounded truth']
    output_dir = './generated-facts'
    os.makedirs(output_dir, exist_ok=True)

    for city in cities:
        try:
            # load ground truth of the city
            file_path = f'./ground-truth/ground-truth-{city}.txt'

            data = load_text_file(file_path)

            # split ground truth by paragraphs, we are going to generate facts from each
            paragraphs = data.split('\n\n')

            # Using OpenAI API to generate facts from a local llm model
            client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")
            llm_model ="lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF"

            facts=[]

            def process(x): # function to process the facts generated by the model (its raw output)
                read_facts = x.split('\n')
                # remove new lines
                read_facts = [fact.replace('\n','') for fact in read_facts]
                # remove empty strings
                read_facts = [fact for fact in read_facts if fact]
                return read_facts
            
            for veracity in veracities:

                # if before iterating the paragraphs the files already exist it means it was already done
                if os.path.exists(f'{output_dir}/generated-facts-{city}-{veracity}.txt'):
                    print(f'{veracity} facts of {city} already done',flush=True)
                else:
                    print(f'Making {veracity} facts of {city}',flush=True)
                try:
                    for ip,p in enumerate(paragraphs):
                        pmin=min(len(p),100)
                        try:
                            output_file = f'{output_dir}/generated-facts-{city}-{veracity}.txt'
                            n = max(len(p.split('.'))-1,1) # number of facts to generate from the paragraph (at least 1 up to the number of sentences - 1)

                            # Main prompt to generate facts
                            prompt =f'Generate a list of {n} {veracity} facts from it. Each fact should be roughly one to two sentences in length. Each sentence should be self contained and make sense individually, that is very important. Do not assume the reader already know which city each fact is referring to. Each fact should have the name of the city {city} somewhere on it and with a maximum of two sentences. Dont announce your answer, just give the list with no numbering, just facts separated by new lines. You are a text generator, not an assistant. Remember that the facts have to be {veracity}.'

                            # Change the prompt based on the veracity

                            ## Fakes
                            if 'fake' in veracity.lower():

                                # We ask for explanations for the fakes to make the model "think it through" (this what only an intuition I had which I tested very little)
                                explain ='YOU MUST PUT inside brackets [reason] at the end a detailed reason of why it is fake for every fact you write.'

                                # We will ask for more clickbait language for the obviously fake facts
                                # We ask for more reasonable language for the subtly fake facts
                                obvious_fake = 'obvious' in veracity
                                if obvious_fake :
                                    obvious_str='Use alarmist or clickbait language or something that someone who has lived the city would know its obviously fake. '+explain
                                else:
                                    obvious_str='Write fake facts but do not make them obviously fake. Your language should be reasonable. '+explain

                                # Indicate the reason for asking for fake facts to avoid the model refusing to generate them
                                prompt = prompt+' I need fake data to study fake detection algorithms. '+obvious_str

                            # Truths
                            # If grounded we ask for the facts to be grounded in the paragraph
                            # else we ask the model to use its own knowledge
                            if 'grounded' in veracity:
                                prompt = f"This is a paragraph about {city} which you can assume to be true, ground your facts from it: {p}\n\n"+prompt
                            else:
                                prompt = f"Use your own knowledge about {city} to answer. " +prompt

                            try:
                                # Propmting the model
                                completion = client.chat.completions.create(
                                model=llm_model,
                                messages=[
                                    {"role": "system", "content": "Follow what the user requests. Do not add suggestions or ask questions about what the user wants at the end. Just do as you are told. DO NOT announce your answer or suggest anything or add explanatory text about your answer nor comments. Here you are not an assistant, you are a text generator."},
                                    {"role": "user", "content": prompt}
                                ],
                                temperature=0.8, # May have been interesting to play with this parameter, but I did not have time
                                n=1
                                )
                                content=completion.choices[0].message.content
                                content=process(content)

                                # Tackle the problem of the model not including the city name in the facts, ideally they should be self contained
                                # We ask the model to include the city name in the fact if it is not already there
                                for i,fact in enumerate(content):
                                    try:
                                        if city.lower() not in fact.lower():
                                            messages=[
                                                {"role": "system", "content": f"You forgot to include the city name ({city}) in the fact. Please include the city name in the fact. DO NOT announce your answer or suggest anything or add explanatory text about your answer nor comments. Here you are not an assistant, you are a text generator."},
                                                {"role": "user", "content": fact}
                                            ]
                                            completion = client.chat.completions.create(
                                            model=llm_model,
                                            messages=messages,
                                            temperature=0.8,
                                            n=1
                                            )
                                            newfact=completion.choices[0].message.content.replace('\n','')
                                            content[i]=newfact
                                            print('posterior inclusion of city name in fact',flush=True)
                                    except Exception as e:
                                        try:
                                            log_error(f'replacing fact from {city} paragraph {ip}')
                                            log_error(city)
                                            log_error(e)
                                            log_error(p[:pmin])
                                            log_error(traceback.format_exc())
                                        except:
                                            pass

                                # Read the facts already generated
                                # and append the new facts to the list
                                # I did this to avoid double new lines between the facts
                                # but probably there was a better way to do this

                                read_facts = []
                                if os.path.exists(output_file):
                                    read_facts = load_text_file(output_file)
                                    # get list of facts by one or more new lines
                                    read_facts = process(read_facts)
                                # append the new fact to the list
                                read_facts.extend(content)

                                # write the facts to the file
                                with open(output_file, 'w',encoding='utf8') as file:
                                    for fact in read_facts:
                                        file.write(fact+'\n')
                            except Exception as e:
                                try:

                                    log_error(city)
                                    log_error(ip)
                                    log_error(veracity)
                                    log_error(p[:pmin])
                                    log_error(e)
                                    log_error(traceback.format_exc())
                                except:
                                    pass
                        except Exception as e:
                            try:
                                log_error(city)
                                log_error(e)
                                log_error(traceback.format_exc())
                            except:
                                pass

                except Exception as e:
                    try:
                        log_error(city)
                        log_error(e)
                        log_error(traceback.format_exc())
                    except:
                        pass
            time.sleep(300) # sleep for 5 minutes for the next city, to let the GPU rest
        except Exception as e:
            try:
                log_error(city)
                log_error(e)
                log_error(traceback.format_exc())
            except:
                pass
    log_file.close()

if MODE=='learn':
    # Ideally you should inspect the generated facts to see if they are good enough
    # or check for sentences that are not facts, self contained or with incorrect length

    ## Importing the necessary libraries
    import pandas as pd
    import nltk
    nltk.download('punkt_tab')
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('averaged_perceptron_tagger_eng')
    import re
    from copy import deepcopy
    from nltk.tokenize import word_tokenize
    from nltk.stem import SnowballStemmer
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    from nltk.corpus import wordnet
    import unicodedata
    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score
    from sklearn.metrics import confusion_matrix
    import matplotlib.pyplot as plt
    from sklearn.preprocessing import FunctionTransformer
    from sklearn.preprocessing import StandardScaler
    from sklearn.model_selection import GridSearchCV
    from sklearn.model_selection import RandomizedSearchCV
    from sklearn.pipeline import Pipeline
    from sklearn.model_selection import StratifiedGroupKFold, GroupKFold
    from sklearn import tree
    from sklearn.svm import SVC
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.naive_bayes import BernoulliNB
    from sklearn.svm import LinearSVC
    

    OUTPUT_PATH = 'learning-output'
    os.makedirs(OUTPUT_PATH,exist_ok=True)

    # Get the list of cities we have ground truth for
    cities = glob.glob(f'./generated-facts/generated-facts-**-**.txt')
    cities = [os.path.splitext(os.path.basename(x))[0].split('-')[-2] for x in cities]
    cities = list(set(cities))

    # Get the list of veracities we generated facts for
    veracities = glob.glob(f'./generated-facts/generated-facts-**-**.txt')
    veracities = [os.path.splitext(os.path.basename(x))[0].split('-')[-1].split('.')[0] for x in veracities]
    veracities = list(set(veracities))

    file_pattern = f'./generated-facts/generated-facts-%city%-%veracity%.txt'

    # Auxiliar function to read the facts generated from this script
    # We need to get rid of the explanations we asked the model to include in the facts

    def load_text_file_yorguin(file_path):
        with open(file_path, 'r',encoding='utf8') as file:
            data = file.readlines()
            for l,line in enumerate(data):
                if '[' in line:
                    data[l] = line.split('[')[0] # get rid of explanations that i asked in the case of fakes
                data[l] = data[l].strip()
            data = [line for line in data if line]
        return data

    # We will now generate a dataframe with the facts generated by the model

    dataframes = []
    for city in cities:
        for veracity in veracities:
            data=load_text_file_yorguin(file_pattern.replace('%city%',city).replace('%veracity%',veracity))
            df = pd.DataFrame(data,columns=['fact'])
            df['city'] = city
            df['veracity'] = veracity
            dataframes.append(df)

    df_learn = pd.concat(dataframes,ignore_index=True)
    shuffledf=lambda df: df.sample(frac=1).reset_index(drop=True) # shuffle to avoid ordering bias from the way veracities are read
    df_learn['source']='yorguin'
    df_learn = shuffledf(df_learn)

    # We will now load the facts from other datasets to use as holdout data for hyperparameter tuning

    other_cities = ['10cities(50)','10cities(250)','berlin']
    def load_text_file_others(file_path):
        with open(file_path, 'r',encoding='utf8') as file:
            data = file.read()
        return data.splitlines()

    dataframes_others = []
    for city in other_cities:
        for veracity in ['fakes','facts']:
            file_pattern = f'other_datasets/%veracity%-%city%.txt'
            data=load_text_file_others(file_pattern.replace('%city%',city).replace('%veracity%',veracity))
            df = pd.DataFrame(data,columns=['fact'])
            df['city'] = city
            df['veracity'] = veracity
            dataframes_others.append(df)

    df_others = pd.concat(dataframes_others,ignore_index=True)
    df_others['source']='others'
    df_others = shuffledf(df_others) # shuffle to avoid ordering bias from the way veracities are read


    # Save one csv per veracity (just to accomodate to the format asked in the assignment)
    for veracity,veracity_label in zip(['fake','tru'], ['fakes','facts']):
        df_learn[df_learn['veracity'].str.contains(veracity)].to_csv(f'{veracity_label}.csv')


    ## Preprocessing for one sentence to check behavior of the pipeline
    example_sentence = 'KYōTO has 1000000000000 square miles, !!! what )as?" a beautiful city, WITH and and and and and a lot of temples and shr1nes.'
    print('example sentence',example_sentence)

    # case conversion
    case_conversion = lambda x: x.lower()
    example_sentence = case_conversion(example_sentence)
    print('case conversion', example_sentence)

    # remove accents
    def remove_accents(input_str):
        nfkd_form = unicodedata.normalize('NFKD', input_str)
        return u"".join([c for c in nfkd_form if not unicodedata.combining(c)])
    remove_accents_foo = lambda x: remove_accents(x)
    example_sentence = remove_accents_foo(example_sentence)
    print('remove accents',example_sentence)

    # remove punctuation
    # this affects characters like ō so I did this after removing accents
    remove_punctuation = lambda x: re.sub(r'[^a-zA-Z0-9\s]', ' ', x)
    example_sentence = remove_punctuation(example_sentence)
    print('remove punctuation',example_sentence)

    # tokenize
    # word_tokenize is a function in the nltk library that uses the recommended word tokenizer.
    example_sentence = word_tokenize(example_sentence)
    print('tokenize',example_sentence)

    # remove non-alpha words
    remove_non_alpha = lambda x: [word for word in x if word.isalpha()]
    example_sentence = remove_non_alpha(example_sentence)
    print('remove non-alpha',example_sentence)

    # remove stopwords
    stop_words = set(stopwords.words('english'))
    remove_stopwords = lambda x: [word for word in x if word not in stop_words]
    example_sentence = remove_stopwords(example_sentence)
    print('remove stopwords',example_sentence)

    # stemming or lemmatization?

    # stemming
    stemmer = SnowballStemmer('english')
    stem_foo = lambda x: [stemmer.stem(word) for word in x]
    stemmed_sentence = stem_foo(example_sentence)
    print('stemming',stemmed_sentence)


    def lemmatize_foo(x) :
        # lemmatization with pos tagging

        # is vital to import this in the function because it is not available in the global scope when parallelizing
        from nltk.corpus import wordnet
        from nltk import pos_tag
        lemmatizer = WordNetLemmatizer()

        def get_wordnet_pos(word):
            """Map POS tag to first character lemmatize() accepts"""

            # First letter of POS_TAG assigned in upper case

            tag = pos_tag([word])[0][1][0].upper()

            # first [0] is the first token received by pos_tag
            # [1] is the pos_tag
            # last [0] is the first character of the pos-tag

            tag_dict = {"J": wordnet.ADJ,
                        "N": wordnet.NOUN,
                        "V": wordnet.VERB,
                        "R": wordnet.ADV}

            return tag_dict.get(tag, wordnet.NOUN) # return wordnet.NOUN if no match


        return [lemmatizer.lemmatize(word,get_wordnet_pos(word)) for word in x]
    lemmatized_sentence = lemmatize_foo(example_sentence)
    print('lemmatization',lemmatized_sentence)

    # The backbone of the preprocessing will be everystep except lemmatization or stemming
    # we will test both to see which one is better
    def prep_sentence(sentence, final_step='lemmatization'):
        pipeline = [case_conversion, remove_punctuation, remove_accents_foo,word_tokenize, remove_non_alpha, remove_stopwords]
        for foo in pipeline:
            sentence = foo(sentence)
        final_steps = {'stemming':stem_foo,'lemmatization':lemmatize_foo}
        end = final_steps.get(final_step,stem_foo)
        sentence = end(sentence)
        return sentence

    # Models we will test
    # I define a dictionary so that I can iterate over the models and their hyperparameters

    CLASS_WEIGHT={0:1,1:1} # we will use this class weight for the models
    model_zoo= {}
    model_zoo['LogisticRegression']={
            'init':dict(penalty='l2',
            dual=False,
            tol=0.0001,
            C=1.0,
            fit_intercept=True,
            intercept_scaling=1,
            class_weight=CLASS_WEIGHT,
            random_state=0,
            solver='lbfgs',
            max_iter=300,
            #multi_class='auto', # this is the default, and is deprecated
            verbose=0,
            warm_start=False,
            n_jobs=1,
            l1_ratio=None),
            'foo':LogisticRegression,
            }
    model_zoo['LinearSVC']={
            'init':dict(
                penalty='l2',
                loss='squared_hinge',
                dual='auto',
                tol=0.0001,
                C=1.0,
                multi_class='ovr',
                fit_intercept=True,
                intercept_scaling=1,
                class_weight=CLASS_WEIGHT,
                verbose=0,
                random_state=42,
                max_iter=1000),
            'foo':LinearSVC
        }
    model_zoo['BernoulliNB']={
            'init':dict(
            alpha=1.0, 
            force_alpha=True,
            binarize=0.0,
            fit_prior=True,
            class_prior=None
        ),
        'foo':BernoulliNB
        }

    def buffer(x):
        return x
    ## Vectorizer to be used
    tfidf_vec_params = dict(input='content',
                    encoding='utf-8',
                    decode_error='strict',
                    strip_accents=None, # already done
                    lowercase=False, # already done
                    preprocessor=buffer, # already done
                    tokenizer=buffer, # already done
                    analyzer='word',
                    stop_words=None, # already done
                    token_pattern=None, # already done
                    ngram_range=(1, 1), # unigrams, test bigrams?
                    max_df=1.0, min_df=1, # keep all
                    max_features=None, # hmmm... to be decided or hyperparameter to be tuned?
                    vocabulary=None, # to be fit
                    binary=False, # TODO:hmmm... to be decided
                    dtype=np.float64,
                    norm='l2', # normalize
                    use_idf=True, # use idf or not
                    smooth_idf=True, # this helps with zero division
                    sublinear_tf=False) # use log normalization

    ## Function to run the backbone of the preprocessing over a list of sentences
    def prep_sentences(list_of_sentences, final_step='lemmatization'):
        return [prep_sentence(sentence, final_step) for sentence in list_of_sentences]

    ## Preprocessing tuning for the pipeline using df_others

    # Encoding the labels from boolean to 1 and 0
    df_others['label']=df_others['veracity'].apply(lambda x: 1 if x=='facts' else 0)

    # Splitting the holdout data (df_others, that came from other students) into two sets
    # 1 set will be used to experiment on the preprocessing (X_prep,y_prep)
    # the other set will be used to experiment on the hyperparameters (X_hyper,y_hyper)

    X_others = df_others['fact'].to_list()
    y_others = df_others['label'].to_list()

    X_prep,X_hyper,y_prep,y_hyper = train_test_split(X_others, y_others, test_size=0.5, shuffle=True, random_state=42, stratify=y_others)

    if not os.path.exists(os.path.join(OUTPUT_PATH,'best_preps.pickle')):
        best_preps={}
        for model in model_zoo.keys():
            pipeline = Pipeline([
            ('prep', FunctionTransformer(prep_sentences, validate=False, kw_args={'final_step':'lemmatization'})),
            ('vectorizer', TfidfVectorizer(**tfidf_vec_params)),
            ('clf', model_zoo[model]['foo'](**model_zoo[model]['init']))])

            prep_param_grid = {'prep__kw_args':[{'final_step':'lemmatization'},{'final_step':'stemming'}], 
                    'vectorizer__use_idf':[True,False],
                    'vectorizer__norm':['l1', 'l2', None],
                    }

            # Perform grid search
            grid_search = GridSearchCV(pipeline, prep_param_grid, cv=5, n_jobs=NJOBS, verbose=3, refit=True) # cv= 5 fold stratified non-shuffled kfold
            grid_search.fit(X_prep, y_prep)

            best_params = grid_search.best_params_
            best_score = grid_search.best_score_

            y_pred = grid_search.best_estimator_.predict(X_hyper)
            acc = accuracy_score(y_hyper, y_pred)

            best_preps[model] = {'best_score':best_score,'best_params':best_params, 'holdout_score':acc,'grid_search':grid_search}
            print(best_preps)

        for model in best_preps.keys():
            print(model,best_preps[model]['best_params'],best_preps[model]['best_score'],best_preps[model]['holdout_score'])
        
        save_dict(os.path.join(OUTPUT_PATH,'best_preps.pickle'),best_preps)
    else:
        best_preps = pickle.load(open(os.path.join(OUTPUT_PATH,'best_preps.pickle'),'rb'))

    ## Hyperparameter tuning for the pipeline using X_hyper,y_hyper that came from other students (df_others)

    # Parameters to be explored

    class_weigths=[1] # vary class weight of True class if you want

    # as before, i use dictionaries to iterate over the models
    param_grid = {}
    param_grid['LogisticRegression'] = {
                'penalty': ['l2','l1']
                ,'C': [0.001,0.01, 0.1, 1,10, 100]
                ,'fit_intercept': [True,False]
                ,'solver': ['saga']
                ,'class_weight': [{0: 1, 1: X} for X in class_weigths]
                ,'tol': [1e-3,1e-2],
                }

    param_grid['LinearSVC'] = dict(
        penalty=['l2','l1'],
        loss=['hinge','squared_hinge'],
        dual=['auto'],
        tol=[0.0001,0.001,0.01],
        C=[0.01,0.1,1,10,100],
        multi_class=['ovr'],
        fit_intercept=[True,False],
        intercept_scaling=[1],
        class_weight=[{0: 1, 1: X} for X in class_weigths],
        verbose=[0],
        random_state=[42],
        max_iter=[100,1000,10000]
    )

    param_grid['BernoulliNB'] = {
            'alpha' : [0.1, 0.5, 1.0, 2.0, 10.0], 
            'force_alpha' : [True,False],
            'binarize' : [0.0, 0.5, 1.0, 1.5, 2.0],
            'fit_prior':[True,False],
            'class_prior' : [None]
    }

    # We will use the best preprocessing for each model to tune the hyperparameters

    if not os.path.exists(os.path.join(OUTPUT_PATH,'best_hyper.pickle')):
        best_hyper={}

        for model in best_preps.keys():
            best_params = best_preps[model]['best_params']
            tfidf_best_params = {k.replace('vectorizer__',''):v for k,v in best_params.items() if 'vectorizer' in k}
            tfidf_vec_params_hyper = copy.deepcopy(tfidf_vec_params)
            tfidf_vec_params_hyper.update(tfidf_best_params)

            pipeline = Pipeline([
            ('prep', FunctionTransformer(prep_sentences, validate=False, kw_args=best_params['prep__kw_args'])),
            ('vectorizer', TfidfVectorizer(**tfidf_vec_params_hyper)),
            ('clf', model_zoo[model]['foo'](**model_zoo[model]['init']))])

            # to avoid data leakage we will split X_hyper,y_hyper into train and test (instead of using X_prep,y_prep)

            X_htrain, X_htest, y_htrain, y_htest = train_test_split(X_hyper, y_hyper, test_size=0.33, shuffle=True, random_state=42, stratify=y_hyper)

            this_grid = copy.deepcopy(param_grid[model])
            this_grid={'clf__'+k:v for k,v in this_grid.items()}
            # Perform grid search
            grid_search = GridSearchCV(pipeline,this_grid , cv=5, n_jobs=NJOBS, verbose=3, refit=True) # cv= 5 fold stratified non-shuffled kfold
            grid_search.fit(X_htrain, y_htrain)

            best_params = grid_search.best_params_
            best_score = grid_search.best_score_

            y_pred = grid_search.best_estimator_.predict(X_htest)
            acc = accuracy_score(y_htest, y_pred)

            best_hyper[model] = {'best_score':best_score,'best_params':best_params, 'holdout_score':acc,'grid_search':grid_search}
            print(best_hyper)


        for model in best_hyper.keys():
            print(model,best_hyper[model]['best_params'],best_hyper[model]['best_score'],best_hyper[model]['holdout_score'])
        
        save_dict(os.path.join(OUTPUT_PATH,'best_hyper.pickle'),best_hyper)
    else:
        best_hyper = pickle.load(open(os.path.join(OUTPUT_PATH,'best_hyper.pickle'),'rb'))

    ## Final training and testing of the models
    # We will use the best preprocessing and hyperparameters for each model to train and test the models
    # And we will use the df_learn to train and test the models

    # Defining some quantities from the confusion matrix
    def foo_FPR(cm):
        tn, fp, fn, tp  = cm
        return 100*fp/(fp+tn) if (fp+tn) != 0 else 0

    def foo_PPV(cm):
        tn, fp, fn, tp  = cm
        return 100*tp/(tp+fp) if (tp+fp) != 0 else 0

    def foo_FNR(cm):
        tn, fp, fn, tp  = cm
        return 100*fn/(fn+tp) if (fn+tp) != 0 else 0

    def foo_NPV(cm):
        tn, fp, fn, tp  = cm
        return 100*tn/(tn+fn) if (tn+fn) != 0 else 0
    
    def foo_TPR(cm):
        tn, fp, fn, tp  = cm
        return 100*tp/(tp+fn) if (tp+fn) != 0 else 0
    
    def foo_TNR(cm):
        tn, fp, fn, tp  = cm
        return 100*tn/(tn+fp) if (tn+fp) != 0 else 0

    def foo_ACC(cm):
        tn, fp, fn, tp  = cm
        return 100*(tp+tn)/(tp+tn+fp+fn) if (tp+tn+fp+fn) != 0 else 0

    def get_metrics(cm):
        return {'FPR':foo_FPR(cm),'PPV':foo_PPV(cm),'FNR':foo_FNR(cm),'NPV':foo_NPV(cm),'TPR':foo_TPR(cm),'TNR':foo_TNR(cm),'ACC':foo_ACC(cm)}

    # We will test different veracities
    # TvsSF: True vs Subtly Fake
    # TvsOF: True vs Obviously Fake
    # GTvsSF: Grounded Truth vs Subtly Fake
    # GTvsOF: Grounded Truth vs Obviously Fake

    task_dict = { # notice position 0 is the positive class and position 1 is the negative class
        'TvsSF' :['true','subtly fake'],
        'TvsOF' :['true','obviously fake'],
        'GTvsSF':['grounded truth','subtly fake'],
        'GTvsOF':['grounded truth','obviously fake']}
    # We will also test different split strategies
    # 1. Split by city (StratifiedGroupKFold)
    # 2. Normal Test-Train split stratified by veracity

    for city in df_learn['city'].unique():
        print(city)
        print(df_learn[df_learn['city']==city]['veracity'].value_counts())

    for task in task_dict.keys():

        for split_type in ['gkf']:

            classes=task_dict[task]
            this_df=df_learn[df_learn['veracity'].isin(classes)]
            this_df['label']=this_df['veracity'].apply(lambda x: 1 if x==classes[0] else 0)
            groups = this_df['city']
            gkf = GroupKFold(n_splits=len(np.unique(groups)))

            for model in model_zoo.keys():


                model_root = os.path.join(OUTPUT_PATH,f"model-{model}_task-{task}_split-{split_type}")
                os.makedirs(model_root,exist_ok=True)

                best_prep = best_preps[model]['best_params']
                best_h = {k.replace('clf__',''):v for k,v in best_hyper[model]['best_params'].items() if 'clf' in k}
                tfidf_best_params = {k.replace('vectorizer__',''):v for k,v in best_prep.items() if 'vectorizer' in k}
                tfidf_vec_params_final = copy.deepcopy(tfidf_vec_params)
                tfidf_vec_params_final.update(tfidf_best_params)
                clf_params = model_zoo[model]['init']
                clf_params.update(best_h)

                pipeline = Pipeline([
                ('prep', FunctionTransformer(prep_sentences, validate=False, kw_args=best_prep['prep__kw_args'])),
                ('vectorizer', TfidfVectorizer(**tfidf_vec_params_final)),
                ('clf', model_zoo[model]['foo'](**clf_params))])


                if split_type == 'gkf':
                    
                    if not os.path.exists(os.path.join(model_root,'aggregated_cm_test.json')):

                        aggregated_cm_test = np.zeros((2,2))
                        for i, (train_index, test_index) in enumerate(gkf.split(this_df.index, this_df.index, this_df['city'])):
                            print(f"Fold {i}:")
                            #print(f"  Train: index={train_index}")
                            print(f"  Train: group={set(this_df['city'].iloc[train_index])}")
                            #print(f"  Test:  index={test_index}")
                            print(f"  Test:  group={set(this_df['city'].iloc[test_index])}")

                            pipeline.fit(this_df['fact'].iloc[train_index], this_df['label'].iloc[train_index]) #Training the model

                            # Evaluate the model
                            test_acc = accuracy_score(this_df['label'].iloc[test_index], pipeline.predict(this_df['fact'].iloc[test_index]))
                            train_acc = accuracy_score(this_df['label'].iloc[train_index], pipeline.predict(this_df['fact'].iloc[train_index]))
                            
                            # Confusion matrix
                            cm_test =  confusion_matrix(this_df['label'].iloc[test_index],  pipeline.predict(this_df['fact'].iloc[test_index]), normalize=None,labels=[0,1])
                            cm_train = confusion_matrix(this_df['label'].iloc[train_index], pipeline.predict(this_df['fact'].iloc[train_index]),normalize=None,labels=[0,1])

                            model_dict = {'model':model,'pipeline':pipeline,'task':task,'split_type':split_type,'fold':i,'test_acc':test_acc,'train_acc':train_acc,'cm_test':cm_test,'cm_train':cm_train}
                            # Save the model
                            fold_path=os.path.join(model_root,f'model-fold-{i}.pickle')
                            save_dict(fold_path,model_dict)
                            print(fold_path)
                            print('Train:',get_metrics(cm_train.ravel()))
                            print('Test:',get_metrics(cm_test.ravel()))

                            aggregated_cm_test+=cm_test

                        # Calculate the aggregated confusion matrix
                        final_metrics=get_metrics(aggregated_cm_test.astype(int).ravel())
                        final_metrics['cm']=aggregated_cm_test.astype(int).tolist()
                        print('Aggregated Test:',final_metrics)
                        save_dict(os.path.join(model_root,'aggregated_cm_test.json'),final_metrics,mode='json')
                    else:
                        print(os.path.join(model_root,'aggregated_cm_test.json'),'already exists')
if MODE=='results':

    import pandas as pd
    import seaborn as sns
    import matplotlib.pyplot as plt
    from itertools import product
    import numpy as np

    OUTPUT_PATH = 'results-output'
    INPUT_PATH = 'learning-output'
    os.makedirs(OUTPUT_PATH,exist_ok=True)

    # Analyze preprocessing results

    # These definitions are just to avoid pickling problems
    prep_sentences = lambda x:x
    buffer = lambda x:x
    best_preps = pickle.load(open(os.path.join(INPUT_PATH,'best_preps.pickle'),'rb'))

    best_preps.keys()
    cvs=[]
    for this_model in best_preps.keys():
        this_model_best_prep = best_preps[this_model]
        this_model_grid = this_model_best_prep['grid_search']
        this_df = pd.DataFrame(this_model_grid.cv_results_)
        this_df['model']=this_model
        cvs.append(this_df)
    df=pd.concat(cvs,ignore_index=True)
    df.columns
    df['param_prep__kw_args'] = df['param_prep__kw_args'].apply(lambda x: x['final_step'][:4])
    df['param_vectorizer__use_idf'] = df['param_vectorizer__use_idf'].apply(lambda x: 'IDF' if x else 'No IDF')
    df['param_vectorizer__norm'] = df['param_vectorizer__norm'].apply(lambda x: 'Norm '+x if x else 'No Norm')

    # boxplot of the results accuracy by preprocessing
    param_title = {
        'param_prep__kw_args':'Word Normalization',
        'param_vectorizer__use_idf':'IDF ',
        'param_vectorizer__norm':'Norm '
    }

    fig, axes = plt.subplots(3, 3, figsize=(6, 6)) 
    combs=list(product(best_preps.keys(),param_title.keys()))
    for iax, comb in zip(enumerate(axes.flatten()), combs):
        i,ax=iax
        model, param = comb
        param_name = param_title[param]
        this_df = df[df['model']==model][['mean_test_score',param]]
        sns.boxplot(data=this_df, x=param, y='mean_test_score', ax=ax,)
        ax.set_xticklabels(ax.get_xticklabels() ,rotation=45)
        if i in [0,1,2]:
            ax.set_title(param_name)
        else:
            ax.set_title('')
        ax.set_xlabel('')
        if i in [0,3,6]:
            ax.set_ylabel(f'{model}\nMean Test Score')
        else:
            ax.set_ylabel('')
    plt.tight_layout()
    fig.savefig(os.path.join(OUTPUT_PATH,'preprocessing_results_boxplot.pdf'))


    best_preps = pickle.load(open(os.path.join(INPUT_PATH,'best_preps.pickle'),'rb'))
    best_prep_dict = {}
    for model in best_preps.keys():
        best_prep_dict[model]={}
        best_prep_dict[model]['best_params'] = best_preps[model]['best_params']

    save_dict(os.path.join(OUTPUT_PATH,'best_preps_per_model.json'),best_prep_dict,mode='json')

    # Do the same for hyperparameters
    best_hyper = pickle.load(open(os.path.join(INPUT_PATH,'best_hyper.pickle'),'rb'))

    best_hyper_dict = {}
    for model in best_hyper.keys():
        best_hyper_dict[model]={}
        best_hyper_dict[model]['best_params'] = best_hyper[model]['best_params']
    
    save_dict(os.path.join(OUTPUT_PATH,'best_hyper_per_model.json'),best_hyper_dict,mode='json')

    ## Analyze tasks results

    instance_pattern = 'model-%model%_task-%task%_split-%split%'
    folder_pattern = os.path.join(INPUT_PATH,instance_pattern).replace('%model%','*').replace('%task%','*').replace('%split%','*')
    folders=glob.glob(folder_pattern)

    instances=[]
    for folder_ in folders:
        folder=os.path.basename(folder_)
        print(folder)
        model = os.path.basename(folder).split('_')[0].split('-')[1]
        task = os.path.basename(folder).split('_')[1].split('-')[1]
        split = os.path.basename(folder).split('_')[2].split('-')[1]
        print(model,task,split)
        scores=[]
        for fold in range(11):
            with open(os.path.join(folder_,f'model-fold-{fold}.pickle'),'rb') as f:
                model_dict = pickle.load(f)
            aggregated_cm_test = readjson(os.path.join(folder_,'aggregated_cm_test.json'))
            aggregated_cm_test['model']=model
            aggregated_cm_test['task']=task
            aggregated_cm_test['split']=split
            aggregated_cm_test['TP']=aggregated_cm_test['cm'][1][1]
            aggregated_cm_test['TN']=aggregated_cm_test['cm'][0][0]
            aggregated_cm_test['FP']=aggregated_cm_test['cm'][0][1]
            aggregated_cm_test['FN']=aggregated_cm_test['cm'][1][0]
            scores.append(model_dict['test_acc'])
            aggregated_cm_test['ACC_distribution']=model_dict['test_acc']
            instances.append(aggregated_cm_test)
        print(aggregated_cm_test)

    df = pd.DataFrame(instances)
    df = df.drop(columns=['split'],inplace=False)
    df['BACC'] = (df['TPR']+df['TNR'])/2
    # plot task (veracity) vs acc for each model

    task_vector = ['GTvsOF','TvsOF','GTvsSF','TvsSF']
    model_vector = ['LogisticRegression','LinearSVC','BernoulliNB']
    task_labels={'GTvsOF':'Grounded Truth vs Obviously Fake','TvsOF':'LLM Truth vs Obviously Fake','GTvsSF':'Grounded Truth vs Subtly Fake','TvsSF':'LLM Truth vs Subtly Fake'}
    model_labels={'LogisticRegression':'Logistic Regression','LinearSVC':'Linear SVC','BernoulliNB':'Bernoulli NB'}

    g = sns.catplot(
        data=df, x="task", y='ACC_distribution', hue="model",
        capsize=.2, palette="YlGnBu_d", errorbar="se",
        kind="point", height=6, aspect=1,order=task_vector,hue_order=model_vector)
    g.despine(left=True)
    g.axes.flat[0].set_title('Task vs Accuracy')
    g.set_xticklabels([task_labels[x].replace(' vs ','\nvs\n') for x in task_vector],rotation=45)
    g.set_ylabels('Accuracy')
    g.set_xlabels('Task')
    g.set_titles('Model: {col_name}')
    # add task legend
    plt.title('Task vs Balanced Accuracy')
    plt.tight_layout()
    sns.move_legend(g, "upper right")
    g.savefig(os.path.join(OUTPUT_PATH,'task_vs_acc.pdf'))

    # Analysis of feature importance

    def get_salient_words(nb_clf, vect, class_ind):
        """Return salient words for given class
        Parameters
        ----------
        nb_clf : a Naive Bayes classifier (e.g. MultinomialNB, BernoulliNB)
        vect : CountVectorizer
        class_ind : int
        Returns
        -------
        list
            a sorted list of (word, log prob) sorted by log probability in descending order.

        From: https://stackoverflow.com/questions/50526898/how-to-get-feature-importance-in-naive-bayes
        """

        words = vect.get_feature_names_out()
        zipped = list(zip(words, nb_clf.feature_log_prob_[class_ind]))
        sorted_zip = sorted(zipped, key=lambda t: t[1], reverse=True)

        return sorted_zip
    # See the most common top-10 features for each model and task combination across all folds
    features_dicts = []
    for model in model_vector:
        for task in task_vector:
            features_dict={}
            features_dict['model']=model

            features_dict['task']=task
            print(model,task)
            instance_pattern = 'model-%model%_task-%task%_split-%split%'
            folder_pattern = os.path.join(INPUT_PATH,instance_pattern).replace('%model%',model).replace('%task%',task).replace('%split%','*')
            folders=glob.glob(folder_pattern)
            print(folders)
            top_features = []
            for folder_ in folders:
                for fold in range(11):
                    with open(os.path.join(folder_,f'model-fold-{fold}.pickle'),'rb') as f:
                        model_dict = pickle.load(f)
                    pipeline = model_dict['pipeline']
                    vectorizer = pipeline.named_steps['vectorizer']
                    if model in ['LinearSVC','LogisticRegression']:
                        clf = pipeline.named_steps['clf']
                        feature_names = np.array(vectorizer.get_feature_names_out())
                        coef = clf.coef_.ravel()
                        top_positive_coefficients = np.argsort(coef)[-10:]
                        top_negative_coefficients = np.argsort(coef)[:10]
                        #top_coefficients = np.hstack([top_negative_coefficients, top_positive_coefficients])
                        #top_features.extend(list(feature_names[top_coefficients]))
                        top_features.extend(list(feature_names[top_negative_coefficients]))
                    elif model == 'BernoulliNB':
                        clf = pipeline.named_steps['clf']
                        feature_names = np.array(vectorizer.get_feature_names_out())
                        negative=[x[0] for x in get_salient_words(clf,vectorizer,0)[:10]]
                        positive=[x[0] for x in get_salient_words(clf,vectorizer,1)[:10]]
                        # obtain the top 10 features for the negative class that are not in the top 10 features for the positive class
                        top_features.extend([x for x in negative if x not in positive])#+[x for x in positive if x not in negative])
                        #top_features.extend([x[0] for x in get_salient_words(clf,vectorizer,1)[:10]])
            features_dict['features']=top_features
            features_dicts.append(features_dict)
    df_features = pd.DataFrame(features_dicts)
    df_features['features_unique'] = df_features['features'].apply(lambda x: np.unique(x,return_counts=True))
    df_features['features_unique'] = df_features['features_unique'].apply(lambda x: {k:v for k,v in zip(x[0],x[1])})
    df_features['features_unique'] = df_features['features_unique'].apply(lambda x: {k:v for k,v in sorted(x.items(), key=lambda item: item[1],reverse=True)})

    for i,row in df_features.iterrows():
        print(row['model'],row['task'])
        print(row['features_unique'])
    
    # make a table of task vs model where each cell is the top 10 features of the combination
    df_features = df_features.drop(columns=['features'],inplace=False)
    df_features['task']=df_features['task'].apply(lambda x: task_labels[x])
    df_features['model']=df_features['model'].apply(lambda x: model_labels[x])
    df_features_ = df_features.pivot(index='task',columns='model',values='features_unique')
    df_features_.to_csv(os.path.join(OUTPUT_PATH,'features_table_dict.csv'))
    df_features_.map(lambda x: ', '.join(x.keys())).to_csv(os.path.join(OUTPUT_PATH,'features_table_list.csv'))
